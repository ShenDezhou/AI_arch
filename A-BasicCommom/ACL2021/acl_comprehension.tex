%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{1367} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Transformer based Reading Comprehension model for China Court Cases}

\author{Dezhou Shen \\
  Department of Computer Science\\
  Tsinghua University\\
  Beijing, CN 100084\\
  \texttt{sdz15@mails.tsinghua.edu.cn} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
 Teaching machines to read court cases remains an elusive challenge.
 In this paper, I proposed an XLNet based model, and use the Capsule Net as hidden representation layers,
 and use multi-task learning for the question type, start-end indices, and supporting sentence indices,
 thanks to the deep neural architecture, the proposed model learns to read real documents and answer complex questions.
 Test on the CJRC2019 validation dataset, proposed model achieved 1.57\% improvement on the span F1-score and 1.35\% on the supporting sentence F1-score comparing to the BERT model.

\end{abstract}

\section{Introduction}

  Progress on the machines reading and understanding documents has been made recently.
  Supervised natural language reading comprehension datasets have been brought recently, e.g. CJRC2019 by \citet{duan2019cjrc}, and CMRC2018 by \citet{cui2019span}.
  \citet{Hermann2015Teaching} observed that summary and paraphrase sentences from the documents can be converted to the context-query-answer pattern.
  In this paper, I emphasize solving the problem that given a context, and a corresponding question, the answer can be generated by the entity detection out of the context,
  or answering the question by affirmative, disagreement, and not-responding.

\section{Related Work}

  \citet{devlin2019bert} proposed Bidirectional Encoder Representations from Transformers(BERT), and achieved 92.8\% on the CoNLL-2003 test set by
fine-tuning BERT on the downstream Named Entity Recognition task.

  \citet{yang2019xlnet} proposed an autoregressive language model which overcomes the limitations of BERT in sequence length,
results showed that XLNet achieved promising results on question answering, natural language inference, sentiment analysis, and document ranking tasks.

  \citet{cui2020revisiting} fine-tuned BERT and XLNet with extra 4.5 billion Chinese tokens, and improved the performance of many downstream tasks.

  \citet{Hermann2015Teaching} proposed LSTM models in building full natural language understanding systems, and achieved accuracy of 63.8\% on the CNN dataset and accuracy of 69.0\% on the Daily Mail dataset.

  \citet{cui2019span} proposed a BERT machine reading comprehension model and achieved the F1-score of 86.0\% on the CMRC2018 test dataset,
and showed that the multilingual pretrained model outperforms the Chinese pretrained model by around 0.8\% in the F1-score on the CMRC2018 test dataset.

  \citet{duan2019cjrc} proposed a BERT based Span-extraction reading comprehension model, and a China court case reading comprehension dataset CJRC2019.

\section{Methodology}
\label{sec:hireachy}

The network model architecture is shown in Figure (1).

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{architecture-cjrc.pdf}}
\end{center}
   \caption{The Machine Reading Comprehension Network Architecture for the China Judicial Reading Comprehension task.}
\label{fig:short}
\end{figure*}

\subsection{The China Judical Reading Comprehension Dataset}

    There are 39333 training records, 6000 development records, and 6000 test records in the CJRC2019 dataset,
and there are 5053 training records and 1565 test records in the CJRC2020 dataset.
    The CJRC2020 followed the same format as this paper proposed, however, the annotation records has limit size, thus, I focus on the revised CJRC2019 dataset.

    For a document with multiple sentences and paragraphs, the first step is splitting the document into lines by carefully choosing the separator token.
    For Chinese sentences, the line-breaks, commas, periods, and semicolons are the most common separator tokens.

    Each record in the dataset has four components: the context, the question, the answer, and the supporting facts segment indices.
    The dataset context-query-answer format has converted to less nested format for clarity, and the context has limited by a maximum size of 512 according to the answer span index.
    And the context was split by separator tokens, and the answer related sentences were tagged in the record,
    and this paper also focus on the problem that finding out the supporting sentences for the answer to the question in the context.


\subsection{Tokenization And Encoding}

For each record, I concatenate the context, and the question then use the BERT tokenizer and converted to BERT token identification sequeces.
Then, the answer's token start and end positions were computed, and the answer type indicating one of the four types: span, affirmative, disagreement, and not-responding.


After the tokenization steps, the tokens are encoded to token integers which represents their vocabulary indices.
To encode by the BERT model, the mask tokens and the segment tokens are prepared.
The non-padding tokens' mask tokens are set to [1], and the segment tokens are set to [0].
The padding tokens' make tokens are set to [0], and the segment tokens are set to [1] for telling the model the boundary of the sentences.
For encoding the sequence, all the layers are preserved for later usage.

The supporting states and the last layer of the BERT encoder were multiplied, and the max value of the sequence was kept,
then connected to a fully connected layer for the supporting sentences.

Then from the output of the start, and the end position tensors, the maximum possible start and end position were calculated
using the positional probability addition of the all possible start-end pairs.

\subsection{Sequence Hidden Representation: The Capsule Net}

After the encoding layers, the hidden layers of the BERT layers are kept, and the last two layers are concatenated and connected to
two Fully Connected layers for the starting index, and the ending index for the answer span indices in the context.

Recall that \citet{Sabour2017Dynamic} proposed the Capsules which are convolutions with the block non-linearity and routing, with fewer parameters than convolutional layers,
try to build better model hierarchical relationships inside of internal knowledge representation.

In general, the Capsule Net architecture in the model contains six components: three convolutional layers, the concatenating layer, the primary capsule layer, the flatten capsule layer,
the compression layer, and the fully connecting capsule layer.

Then last hidden layer of the BERT encoder was retrieved and then connected to the Capsule Net.
The Capsule Net has three dimensions, representing different grains for the features, the parameters for the channels are 2, 4, and 8.
Then for each channel, the convolutional layer with input channel size of 512, output channel of 32, convolutional kernel is the feature kernel,
that's 2, 4, 8, and the stride size of 2.
After the convolutional layers, the three output layers are concatenated,
then connects to the primary capsule layer with capsule number of 128, input channel size of 32, output channel size of 32, kernel size and stride size of 1.
Then a flatten-capsule layer, a compression layer, and a fully-connected capsule layer were connected.
Finally, a fully connected layer was used for the result type indicating the answer type is a span or affirmative, disagreement, and not-responding.



\subsection{Loss Function And Training Hyper Parameters}

The experiment was performed on a Google Cloud NVIDIA V100 with 16GB of the high speed of memory each, with 32GB of RAM,
which can provide 112 tera-flops of computation capability.


Three losses were added with weight and then use the stochastic gradient descent to train the network parameters.
The start position loss, the end position loss, the type loss, and the supporting sentence loss were added with weighted.
The start and end loss use the Cross Entropy Loss function, the type uses binary cross entropy with logits,
and the support loss used the binary cross entropy with logits between the output and the gold value, then get the mean of each record.
The support loss has a weight of 5 for its importance in finding the destination answer span, and all other loss has the weight of 1.
And the threshold for the supporting sentence is 0.5.
The learning rate is $5e{-5}$ for the RBT3 and XLNet model, learning rate is $5e{-4}$ for the RoBERTa model, the dropout rate is 5\%, the document max sequence length for the document(the question and the context) is 512, and the max sequence length for the question is 45, the epoch size is 10.
The batch size is 16 for the RBT3, RoBERTa, XLNet model.

For the RoBERTa and XLNet model, I use the gradient accumulation steps for the training, the loss are accumulated between batches,
and every two batches, the backward result was calculated.
In this way, the large pretrained models, such as RoBERTa and XLNet, were able to be trained on the Nvidia V100 16GB of memory.

\section{Experiments}

\subsection{Dataset}

 The CJRC2019 dataset proposed by \citet{duan2019cjrc} lacks the supporting sentence annotations.
For the multiple questions in one paragraph, I use the answer starting position as anchors to retrieve
the attentional subparagraph, and use the 512 characters subparagraph containing the answer as the revised context.
Also, I use the answer occurrence in the sentence as the flag to annotate the supporting sentences.

\subsection{Results}


As Table (1) shows, on the CJRC2019 development dataset, the RBT3 model achieved 39.65\% of the F1 score on the span and 36.84\% of the F1 score on the supporting sentence.
And the RoBERTa model improved performance significantly for the 12 layers of transformers, comparing to RBT3's 3 layers,
achieved 44.37\% of the F1 score on the span and 48.00\% of the F1 score on the supporting sentence.
The XLNet model achieved the best performance with 45.94\% of th F1 score on the span, and 49.35\% F1 score on the supporting sentence.


\begin{table}
\centering
\begin{tabular}{cccc}
\hline
\textbf{Model} & \textbf{Layers}& \textbf{span-F1} & \textbf{sup-F1} \\
\hline
RBT3 & 3 & 39.65\% & 36.86\% \\
RoBERTa & 12 & 44.37\% & 48.00\% \\
XLNet & 12 & 45.94\% & 49.35\% \\
\hline
\end{tabular}
\caption{Comparison of the pre-trained language models on the CJRC2019 Machine Reading Comprehension validation task.}
\end{table}



%\subsection{Reproducibility}
%
%The data augment, the LSTM, RoBERTa, XLNet, CRF codes and pre-trained Chinese word segment models are available at https://drive.google.com/drive/folders/1--GwAkEPtQsIm0PqD6a26TOvVrxJg3mR.
%And the supplementary codes are available at https://github.com/ShenDezhou/EXLNet.

\section{Conclusion}

In this paper, I introduced a novel Chinese Reading Comprehension model for Court Cases, which leveraging hierarchical layers of the document encoding layers.
By making use of the Capsule Net layers, the multi task model predicts the starting and ending span index, and the supporting sentence indices in the context.
Proposed model achieved 1.57\% improvement on the span F1-score and 1.35\% on the supporting sentence F1-score comparing to the BERT model in the CJRC2019 validation dataset.


\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021,acl_comprehension}

%\appendix



\end{document}
