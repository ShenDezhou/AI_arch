%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{509} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Transformer based Reading Comprehension model for China Court Cases}

\author{Dezhou Shen \\
  Department of Computer Science\\
  Tsinghua University\\
  Beijing, CN 100084\\
  \texttt{sdz15@mails.tsinghua.edu.cn} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
 Teaching machines to read court cases remains an elusive challenge.
 In this paper, I proposed a Transformer based model that learn to read real documents and answer complex questions.
 Test on the CJRC2019 dataset, proposed model achieved 2\% improvement on the macro-F1 score compared to the BERT model.

\end{abstract}

\section{Introduction}

  Progress on the machines reading and understanding documents has been made recently.
  Supervised natural language reading comprehension datasets have been brought recently, \eg \citet{duan2019cjrc}, \citet{cui2019span}.
  \citet{Hermann2015Teaching} observed that summary and paraphrase sentences from the documents can be converted to the context-query-answer pattern.
  In this paper, I emphasize solving the problem that given a context, and a corresponding question, the answer can be generated by the entity detection out of the context,
  or answering the question by affirmative, disagreement, and not-responding.

\section{Related Work}

  \citet{devlin2019bert} proposed Bidirectional Encoder Representations from Transformers(BERT), and achieved 92.8\% on the CoNLL-2003 test set by
fine-tuning BERT on the downstream Named Entity Recognition task.

  \citet{yang2019xlnet} proposed an autoregressive language model which overcomes the limitations of BERT in sequence length,
results showed that XLNet achieved promising results on question answering, natural language inference, sentiment analysis, and document ranking tasks.

  \citet{cui2020revisiting} fine-tuned BERT and XLNet with extra 4.5 billion Chinese tokens, and improved the performance of many downstream tasks.

  \citet{Hermann2015Teaching} proposed LSTM models in building full natural language understanding systems, and achieved accuracy of 63.8\% on the CNN dataset and accuracy of 69.0\% on the Daily Mail dataset.

  \citet{cui2019span} proposed a BERT machine reading comprehension model and achieved the F1-score of 86.0\% on the CMRC2018 test dataset,
and showed that the multilingual pretrained model outperforms the Chinese pretrained model by around 0.8\% in the F1-score on the CMRC2018 test dataset.

  \citet{duan2019cjrc} proposed a BERT based Span-extraction reading comprehension model, and a China court case reading comprehension dataset CJRC2019.

\section{Methodology}
\label{sec:hireachy}

\subsection{The China Judical Reading Comprehension Dataset}

    There are 39333 training records, 6000 development records, and 6000 test records in the CJRC2019 dataset,
and there are 5053 training records and 1565 test records in the CJRC2020 dataset.

    For a document with multiple sentences and paragraphs, the first step is splitting the document into lines by carefully choosing the separator token.
    For Chinese sentences, the line-breaks, commas, periods, and semicolons are the most common separator tokens.

    Each record in the dataset has four components: the context, the question, the answer, and the supporting facts segment indices.
    The dataset context-query-answer format has converted to less nested format for clarity, and the context has limited by a maximum size of 512 according to the answer span index.
    And the context was split by separator tokens, and the answer related sentences were tagged in the record,
    and this paper also focus on the problem that finding out the supporting sentences for the answer to the question in the context.


\subsection{Tokenization And Encoding}

For each record, I concatenate the context, and the question then use the BERT tokenizer and converted to BERT token identification sequeces.
Then, the answer's token start and end positions were computed, and the answer type indicating one of the four types: span, affirmative, disagreement, and not-responding.


After the tokenization steps, the tokens are encoded to token integers which represents their vocabulary indices.
To encode by the BERT model, the mask tokens and the segment tokens are prepared.
The non-padding tokens' mask tokens are set to [1], and the segment tokens are set to [0].
The padding tokens' make tokens are set to [0], and the segment tokens are set to [1] for telling the model the boundary of the sentences.
For encoding the sequence, all the layers are preserved for later usage.

The supporting states and the last layer of the BERT encoder were multiplied, and the max value of the sequence was kept,
then connected to a fully connected layer for the supporting sentences.

Then from the output of the start and the end position tensors, the maximum possible start and end position were calculated
using the positional probability addition of the all possible start-end pairs.

\subsection{Sequence Hidden Representation: The Capsule Net}

After the encoding layers, the hidden layers of the BERT layers are kept, and the last two layers are concatenated and connected
two Fully Connected layers for the starting index and the ending index for the answer span indices in the context.

Then last hidden layer of the BERT encoder was retrieved and then connected to the Capsule Net.
The Capsule Net has three dimensions, representing different grains for the features, the parameters for the channels are 2, 4, and 8.
Then for each channel, the convolutional layer with input channel size of 512, output channel of 32, convolutional kernel is the feature kernel,
that's 2, 4, 8, and the stride size of 2.
After the convolutional layers, the three output layers are concatenated,
then connects to the primary capsule layer with capsule number of 128, input channel size of 32, output channel size of 32, kernel size and stride size of 1.
Then a flatten-capsule layer, a compression layer, and a fully-connected capsule layer were connected.
Finally, a fully connected layer was used for the result type indicating the answer type is a span or affirmative, disagreement, and not-responding.



\subsection{Loss Function And Training Hyper Parameters}

Three losses were added with weighted and then use the stochastic gradient descent to train the network parameters.
The start position loss, the end position loss, the type loss, and the supporting sentence loss were added with weighted.
The start and end loss use the Cross Entropy Loss function, the type uses binary cross entropy with logits,
and the support loss used the binary cross entropy with logits between the output and the gold value, then get the mean of each record.
The support loss has a weight of 5 for its importance in finding the destination answer span, and all other loss has the weight of 1.
And the threshold for the supporting sentence is 0.5.
The learning rate is $5e{-4]}$, the dropout rate is 5\%, the document max sequence length for the document(the question and the context) is 512, and the max sequence length for the question is 45, the epoch size is 10.
The batch size is 16,8, or 4 for the RBT3, RoBERTa-wwm-ext, and RoBERTa-Large model, for the GPU memory limit.

The experiment was performed on a Google Cloud NVIDIA V100 with 16GB of the high speed of memory each, with 32GB of RAM,
which can provide 112 tera-flops of computation capability.


\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{cws-architecture.pdf}}
\end{center}
   \caption{The Network Architecture for the Chinese word segment task.}
\label{fig:short}
\end{figure*}

\section{Experiments}

\subsection{Dataset}

 The MSR Chinese word dataset has 2.37 million words in the training set with 88 thousand vocabulary size,
 and has 107 thousand words in the test set with 13 thousand vocabulary size.
 As \citet{emerson2005second} reported that, during the second international Chinese word segment competition,
the best F1-score achieved was 97.2\% with permission of using the additional corpus on the MSR test dataset.

\subsection{Common Setups}

\begin{table}
\centering
\begin{tabular}{cccc}
\hline
\textbf{Model} & \textbf{Neurons}& \textbf{Layers} & \textbf{F1} \\
\hline
RBT3 & 768 & 3 & 92.62\% \\
RoBERTa & 768 & 12 & 94.65\% \\
RoBERTa-Large & 1024 & 24 & 97.88\% \\
\hline
\end{tabular}
\caption{Comparison of the pre-trained language models on the MSR2005 Chinese word segement task.}
\end{table}


All the Table (1) models share almost the same network architecture except for the embedding of the tokens.

%\subsection{Reproducibility}
%
%The data augment, the LSTM, RoBERTa, XLNet, CRF codes and pre-trained Chinese word segment models are available at https://drive.google.com/drive/folders/1--GwAkEPtQsIm0PqD6a26TOvVrxJg3mR.
%And the supplementary codes are available at https://github.com/ShenDezhou/EXLNet.

\section{Conclusion}

In this paper, I introduced a novel Chinese word segment architecture, which leveraging hierarchical layers of the document encoding layers.
By keeping the Conditional Random Field layer, I proposed a novel sequence hidden representation of the document sequence, which achieved 3.23\% performance gain in macro F1-score than
the BERT and CRF architecture on the MSR2005 Chinese word segment test dataset.
Not surprisingly, the performance of the proposed XLNet-CRF model also exceed the best result of the Second International Chinese Word Segmentation Competition.


\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021,acl_word}

%\appendix



\end{document}
