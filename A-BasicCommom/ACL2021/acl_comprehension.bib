@inproceedings{duan2019cjrc,
  title={Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension},
  author={Duan, Xingyi and Wang, Baoxin and Wang, Ziyue and Ma, Wentao and Cui, Yiming and Wu, Dayong and Wang, Shijin and Liu, Ting and Huo, Tianxiang and Hu, Zhen and others},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={439--451},
  year={2019},
  organization={Springer}
}

@inproceedings{cui2019span,
  title={A Span-Extraction Dataset for Chinese Machine Reading Comprehension},
  author={Cui, Yiming and Liu, Ting and Che, Wanxiang and Xiao, Li and Chen, Zhipeng and Ma, Wentao and Wang, Shijin and Hu, Guoping},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5886--5891},
  year={2019}
}

@inproceedings{Hermann2015Teaching,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2021 Elsevier Inc.},
copyright = {Compendex},
title = {Teaching machines to read and comprehend},
journal = {Advances in Neural Information Processing Systems},
author = {Hermann, Karl Moritz and Koisky, Toma and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
volume = {2015-January},
year = {2015},
pages = {1693 - 1701},
issn = {10495258},
address = {Montreal, QC, Canada},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.<br/>},
key = {Large dataset},
keywords = {Deep neural networks;Natural language processing systems;},
note = {Complex questions;Language structure;Natural languages;Prior knowledge;Reading comprehension;},
}



@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5753--5763},
  year={2019}
}


@inproceedings{cui2020revisiting,
    title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
    author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
    booktitle = "Findings of EMNLP",
    year = "2020",
    publisher = "Association for Computational Linguistics"
}

@inproceedings{Sabour2017Dynamic,
title = {Dynamic routing between capsules},
journal = {Advances in Neural Information Processing Systems},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
volume = {2017-December},
year = {2017},
pages = {3857 - 3867},
issn = {10495258},
address = {Long Beach, CA, United states}
}



