%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{A Transformer-based Pretrained Language Model for Chinese Law Examination}

\author{Dezhou Shen \\
  Department of Computer Science\\
  Tsinghua University\\
  Beijing, CN 100084\\
  \texttt{sdz15@mails.tsinghua.edu.cn} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
 Teaching machines to take examinations of the National Judicial Examination of China is a meaningful task.
 However, until now perfect solution for the Question Answering is still missing.
 In this paper, I demonstrate a novel model which leverage the Transformers and Convolutional Neural Networks for the Judicial Examination task,
    propose model achieved 35\% in accuracy.
\end{abstract}

\section{Introduction}

  Legal Question Answering aims to provide explanations, advice and solutions for legal issues.
Reading Comprehension and Open-domain Question Answering are two of the most studied topics in the natural processing language.



\section{Related Work}

\citet{hermann2015teaching} found that the combination of the Deep LSTM encoders, and the Attention model works better on the Daily Mail reading comprehension dataset,
 and by document representation with the token-level question representation the Attention model achieved better result on the CNN reading comprehension dataset.

\citet{hashemi2019performance} proposed an efficient evaluation method for the question answering, compared to the extensive study in
the retrieval tasks, the proposed neural model for evaluating query performance prediction showed that its outstanding in all the methods in the QA domain.

\citet{huang2019knowledge} borrowed idea from the knowledge graph completion and recommender system, they use the low-dimensional vector learned from knowledge graph,
then trained the question answering model.

\citet{karki2019question} broke the question answering problem into several subtasks, such as table retrieval and element classification.
the performace of the translation from a question to SQL clause is still far from satisfactory, with 15.6\% to 19.5\% in the F1 score on the test dataset.

\citet{mcelvain2019non} showed that for the non-factoid question answering task in the legal domain, legally correct, jurisdictional relevant, and conversationally responsive answers
are three key factors.

\citet{jin2019video} described that by understanding the question and the contextual images, the Attention model learn the answers, the total accuracy achieved 30.2\% in the YouTube-QA Dataset.

\citet{pizzato2008indexing} proposed the weight based bag-of-words, partial relation and full relation to model semantic roles for questions as labels in the training procedure.

\citet{zhan2020medical} proposed an Attention-based reasoning model in the medical visual question answering.

\citet{qu2020open} studied a transformer-based conversational QA system, and use retrieval and rerank pattern to solve the problem.

\section{Methodology}
\label{sec:hireachy}



\subsection{Legal Question Answering Dataset}

I use a dataset collected by Zhong \etal, which contains 7775 knowledge-based QA pairs and 13297 case-analysis QA pairs.
Some questions have multiple options correct in the answer, while some have single option correct.


\subsection{Context Representation}

There are three descriptive information, namely question, options, and paragraphs for each item in the dataset.
The paragraph is optional for the knowledge based QA-pair and mandatory for the case analysis QA-pair.
The paragraph and the question always come together in a single paragraph.
There are always four options, and the correct options are not variant according to the question.

I use the BERT model as the representation for the question and option encoding.
In total, seven different BERT pre-trained language model were compared.
Google BERT for Chinese, Roberta BERT for Chinese, Roberta BERT 3 layers for Chinese,
Roberta BERT 3 layers for Chinese(1024 dim), Roberta BERT 24 layers for Chinese (1024 dim),
Extra Roberta BERT 24 layers for Chinese (1024 dim), Extra More Steps Roberta BERT 24 layers for Chinese (1024 dim).


\subsection{Sequence To Sequence Summarization}

For the token generation in the summarization step, I use a sequence-to-sequence model.
By tagging each token existence in the corresponding summary, a sentence has its unique sequence of code for the token importance.
For the 14,908 samples of important sentences for training and 108 samples for test in the Legal Token Summarization Dataset,
I design a token summarization model as follows:
First, encode the sentence by RoBERTa language model, and concatenate the first and last layers as the sentence representation.
Then concatenate to a CapsuleNet layer, then concatenates the output layer.

The learning rate is $10^{-3}$, and the bach size is 2, and the accumulation step is 8.
The training epoch is 10, and the accuracy of the best model in the test set is 42.05\%.
From Table(1) we can see that different number of transformer layers has no significant performance improvement for accuracy in the test set.
The RBT3, BRT3L and RoBERTa vary in number of neurons and number of layers,
which was popular pre-trained language models trained on 4.5 billion of Chinese tokens by \citet{cui2020revisiting}.

\begin{table}
\centering
\begin{tabular}{cccc}
\hline
\textbf{Model} & \textbf{Neurons}& \textbf{Layers} & \textbf{Accuracy} \\
\hline
RoBERTa & 768 & 12 & 42.05\% \\
RBT3 & 768 & 3 & 40.18\% \\
RBT3L & 1024 & 3 & 41.22\% \\
\hline
\end{tabular}
\caption{Pre-trained language models accuracy in the sequence-to-sequece training steps.}
\end{table}


For balancing the readability and quality of the summary, I did not use the sequence predicted by the token summarization model directly.
Instead, a bigger threshold for the output of the model can improve the readability and the quality.
As a practical result, I choose 87.5\% as the threshold for keeping the important token.
That is to say, keeping the most important 87.5\% tokens for each inference shall get the best summarization result.

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{SummaryModel.pdf}}
\end{center}
   \caption{Different architectures for multi-grained summarization.
   (a) Extractive Sentence Summarization,
   (b) Sequence To Sequence Summarization.}
\label{fig:short}
\end{figure*}

\subsection{Summarization Evaluation}

\citet{lin2004rouge} represents a set of similar metrics such as ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU.
In this paper, I use ROUGE-1, ROUGE-2, ROUGE-L and ROUGE-W as the summarization evaluation metrics.
The ROUGE-W score is computed using Formula (1).

\begin{equation}
  {R_W} = {R_1} * 0.2 + {R_2} * 0.4 + {R_L} * 0.4
\end{equation}

\begin{table}
\centering
\begin{tabular}{cccc}
\hline
\textbf{R-1} & \textbf{R-2} & \textbf{R-L} & \textbf{R-W} \\
\hline
32.62\% & 15.63\% & 31.79\% & 25.49\% \\
\hline
\end{tabular}
\caption{F-score for text summarization in the dataset.}
\end{table}

\section{Conclusion}

In this paper, I introduced a novel summarization method, which leveraging hierarchical features of the document.
By extracting multi-grained level from the document, the summary contains the paragraph-level, sentence-level and token-level key information.
The evaluation results showed that the multiple layers of transformers have improved the Rouge evaluation performance
and achieved a Rouge-W score of 25.49\% on the Chinese Legal Case Abstraction dataset.


\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021,acl_examination}

%\appendix



\end{document}
