% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
% Include other packages here, before hyperref.


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{8182} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Text Detection and Chinese Character Recognition in Natural Scene Images via Pre-trained Vision and Language Models}

\author{Dezhou Shen\\
Department of Computer Science\\
Tsinghua University\\
Beijing, CN 100084\\
{\tt\small sdz15@mails.tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}

  Detecting and reading text from natural images is an essential computer vision tasks.
  And Chinese characters have more tokens than English characters, thus OCR in Chinese is harder than English.
  I encode high dimension features extracting from images by ResNet by the pre-trained language models of
  \underline{B}idirectional \underline{E}ncoder \underline{R}epresentations from \underline{T}ransformers,
  then connect to a recurrent neural network layer, and a fully connected layer for Character Recognize architecture
  the classification model outperforms the ResNet model and the DenseNet image classification model,
  and achieved accuracy of $80.0\%$ on the Baidu2019 street scene test set.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

  Reading text from photographs is essential to Natural language processing and Computer Vision.
  OCR of printed documents and recognition of hand-written digits are already known as solved problems,
  however, recognizing characters in natural scenes is more difficult.
  Regarding to the fact that Chinese language has more characters than English, thus it is more difficult to get a satisfactory model.
  Most researches have laid lack of attention on the relevance between the Natural language processing and the Computer Vision.
  In this paper, I discussed a novel method by unifying the representation of languages and visions from the representation layer.
  Unsupervised pre-training is important to the modern research in deep learning.
  Lee \etal~\cite{lee2009convolutional} used the pre-training approaches
  in Computer Vision tasks in 2009, and later from 2010 to 2016,
  Nair and Hinton~\cite{nair2010rectified} proved that the pre-training process is supplementary in the Computer Vision tasks,
  thus, can be omitted in some cases.
  However, it started to flourish the natural language processing domain since Mikolov \etal~\cite{mikolov2013distributed} had proposed Word2Vec.
  Not long before Devin \etal~\cite{devlin2019bert}'s Bidirectional Encoder Representations from Transformers language model dominates
  in most frequently used tasks in the natural language processing domain,
  which is close resemble of Vicent \etal~\cite{vincent2008extracting}'s denoising autoencoder model, which was initially designed for images.
  Therefore, the pre-training process becomes one of the most important procedures in deep learning.

%-------------------------------------------------------------------------

\section{Recent Work}

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{detectionnet.pdf}}
\end{center}
   \caption{Combination of the ResNet, Feature Pyramid Net and Differentiable Binarization Net for scene text detection task.}
\label{fig:short}
\end{figure*}

\subsection{Detection}

  Wang \etal~\cite{wang2012end} used CNN as detection model and achieved accuracy of 84\% by word-level on the ICDAR 2003~\cite{lucas2003icdar} test dataset and Street View Text~\cite{wang2011end} dataset.
  LeCun \etal~\cite{lecun1990handwritten}'s first experiment on handwritten digit recognition, CNN's great potential had been revealed.
  Liao \etal~\cite{liao2020real} proposed a Differentiable Binarization module for segmentation task for text detection, and achieved $82.8\%\sim84.9\%$ of F1 score on the TD500~\cite{yao2012detecting} dataset
   and $81.0\%\sim83.4\%$ on the CTW1500~\cite{liu2019curved} dataset.

\subsection{Recognition}

  Chen \etal~\cite{chen2020generative} trained image representation by sequence Transformers and tested on CIFAR-10 to show it is outperforming to Wide-ResNet
which was inspired by unsupervised natural language representation learning.
  Wang \etal~\cite{wang2019development} reviewed that convolutional neural networks had been proposed in the 1960s, and had its implementation in the 1980s.
  In the 2010s, Krizhevsky \etal~\cite{krizhevsky2012imagenet} proposed the deep architecture, AlexNet, by concatenating multiple components of CNN layers.
  Netzer \etal~\cite{netzer2011reading} used unsupervised feature learning K-means to classify the digits cropped from Street View images and achieved accuracy of 90.6\% in the test dataset.
  A lot of variants of AlexNet had been proposed by researchers and the accuracy of ImageNet had been greatly improved, \eg ZFNet~\cite{zeiler2014visualizing}, VGG~\cite{simonyan2014very}, GoogLeNet~\cite{szegedy2015going}, ResNet~\cite{he2016deep},
  ResNeXt~\cite{xie2017aggregated}, inception-ResNet-v2~\cite{szegedy2016inception}, DenseNet~\cite{huang2016deep}.
  Lin \etal~\cite{lin2017feature} proposed multi-scale convolutional network in feature extraction, and the average precision on the COCO dataset is 56.9\%.

\subsection{Representation of Images}

  Lu and Weng~\cite{lu2007survey} concluded that for the multi-source image classification tasks, additional information such as signatures, texture, context, and ancillary data can be combined to achieve better performance.
And it is difficult in handling the dichotomy between pixels and natural language texts in a single model.
  Cui \etal~\cite{cui2020revisiting} proposed several whole-word-masking pre-trained Chinese language models,
which are improved versions of BERT~\cite{devlin2019bert} pre-trained language models, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large.
These models achieved better performance in Chinese machine reading comprehension, Chinese document classification, and other downstream natural language tasks.
  He and Peng~\cite{he2017fine} combined the vision stream and language stream as two parallel channels for extracting multi-source information in the image classification task,
and tested on the CUB-200-2011 image dataset and achieved $85.55\%$ by combining GoogLeNet~\cite{szegedy2015going} and CNN-RNN~\cite{reed2016learning}, the result outperformed many competitors.

%-------------------------------------------------------------------------

\section{Text Detection Models}



  Recent scene text detection methods can be classified into Regeression-based methods and Segementation-based methods.
  By directly regressing the boxes of the text instances, Xie \etal~\cite{xie2019scene} proposed a dimention-decomposition region to handle scale problem in scene text detection.
  By combining pixel-level prediction and post-precessing, Tian \etal~\cite{tian2019learning} proposed pixel embeddings to clusters for the segmentation results.

\subsection{Image Pre-processing}

  During the training phase, the region of interest in the images will be augmented by left-right flipping, image affine, size resize, random crop and border shrinks.
  These augmentation makes the model more robust and transferable from training dataset to test dataset.

\subsection{Model Architecture}
  As Figure (1) shows, I combined the Residual Net, Feature Pyramid Network, and Differentiable Binarization network for text detection task.




%-------------------------------------------------------------------------

\section{Text Character Recognition Models}

  Image are segmented by the detection model, then each segmentation is preprocessed to a fix size of $120\times32$.
The recogonition model takes the adjusted image as input, and predict the sequence of Chinese Character dictionary index as output.


\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{resbertnet.pdf}}
\end{center}
   \caption{Combination of the Residual Net, Bert, Long-Short Term Memory and Fully Connection Net for Chinese character recognition task.}
\label{fig:short}
\end{figure*}

%-------------------------------------------------------------------------
\subsection{Model Architecture}

 As Figure (2) shows, I use a combination of Residual Net, Bert, Long-Short Term Memory and Fully Connection Net for character recognition task.

The model architecture has five functional layers:

\begin{itemize}
\item {\bf Input layer}
\item {\bf Residual Bert layer}
\item {\bf Sequence Decode layer}
\item {\bf Fully Connected layer}
\item {\bf Output layer}
\end{itemize}

All images are resized to a resolution of 120x32 pixels.
Encode the image by the sequence of RGB channel values, in the order of Red-channel, Green-channel, and Blue-channel.
In the Residual Bert  layer, the pixel-channel value are extracted by the residual convolutional network,
then fed into the Bert language model to encode each sub-pixels.
In the Sequence Decode layer, two layers of Long-Short Term Memory networks to decode each Chinese character meanings.
In the Fully Connected layer, extracted tokens are connected to output layer.
In the Output layer, the encoded Chinese character index sequence are feed in the model.



\subsection{The Sub-pixel Encoder}

My approach consists of the feature extraction stage, the sequence decode stage, and the final character mapping state.
In the feature extraction stage, it is intuitive to use the Bert encoder from the pixel level of the image.
However, due to the computational resource limit and the time limit,
It is wise to encode at sub-pixel level.
To be specific, it is practical to encode after the Residual Network extraction, then use the Bert model to encode the result.

The BERT objectives and the sequence Transformer architecture to predict the language tokens.

\par Given an unlabeled dataset $\Bbb X$, the BERT objective samples a sub-sequence $\Bbb S \in \{C\}$,
$C$ represents all possible tokens, and such that for each index $i \in \Bbb S$,
there is independent probability $15\%$ of appearing in $\Bbb S$,
name the token $M$ as the BERT mask.
As equation (1), train the language model by minimizing the BERT objective of the "masked" elements $x_M$
conditioned on the "unmasked" ones $x_{\left[1,n\right]\backslash M}$.

\begin{equation}
  \lambda = \mathop{\Bbb E}\limits_{x\sim\Bbb X} \mathop{\Bbb E}\limits_{M} \sum_{i\in S}{\bigl[-\log{p(x_i\mid x_{\left[1,n \right]\backslash M})\bigr]}}
\end{equation}

\par The transformer decoder takes the image pixels and meta characters sequence $x_1,\cdots,x_n$ and produces a $d$-dimensional
embedding for each position.

For the per-pixel image classification approach, for every RGB channel of pixels in an image,
each pixel had its pixel-channel code, ranges from 0x00 to 0xff for different colors.
Thus, taking these pixels in an image is identical to ASCII characters in a document.
Generality speaking, the performance of the pre-trained language models achieved in the document classification tasks, can be transferred to image classification naturally.

\par Recall that Kim~\cite{kim2014convolutional} had proved that unsupervised pre-trained language model $word2vec$ and CNN outperformed many other machine learning algorithms,
\eg support vector machines and conditional random field, in many datasets such as movie reviews, Stanford sentiment treebank, and TREC question.
Cui \etal's pre-trained language model, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large, had improved performances over many other machine learning algorithms.
BERT and RoBERTa-wwm-ext-large models both achieved an f1-score of 97.8\% in the THUCNews dataset, which contains 65 thousands of news in 10 domains.

\par From table (1), by combining the pre-trained language model with fully connected layer as the document classification model,
the test accuracy exceeds the other popular machine learning algorithms.
Therefore, the pixel channels of an image can be properly represented by these pre-trained language models.


\begin{equation}
  \sigma(\textbf{z})_j = {e^{\textbf{z}_j} \over {\sum_{k=1}^{\textit{K}} e^{\textbf{z}_k}}}, j=1,\cdots,\textit{K}.
\end{equation}

%-------------------------------------------------------------------------


\section{Experiments and Results}


\subsection{Training}

The Baidu Street View character dataset contains 188560 annotated images and from these randomly choose 892 images as the test set.

As Cui \etal~\cite{cui2020revisiting} reported that the training of the RBT3 language model was based on Devlin \etal's model,
moreover, the pre-trained Chinese language models use extra 5 billion training tokens comparing to Devlin \etal's 0.4 billion tokens.
I use a batch size of 32 and train for $10$ epochs using AdamW with $\beta_1=0.99$, $\beta_2=0.999$, and weight decay of $10^{-8}$.
I set the learning rate to $10^{-4}$ and no warmed up or decay are used.

\par The training time for each epoch batch size is 136ms, comparing to the 20ms for one epoch in the legacy model without the Bert encoding layer.
Thus, the time cost for the training increased 5.8 times of before.

\par The experiment was performed on a Google Cloud Nvidia v100, with 32GB of CPU RAM, and with 16GB of GPU memory,
which can provide 125 tera-flops of computation capability.

\subsection{Analysis}

Use the proposed model, and I tried different pre-trained language models to see the impact on classification accuracy.
From the table (1), it can be seen that the models are trained on the training dataset and validated on the test dataset.
For the same size of a dataset with larger classes, it needs more epochs and training time for the classification model.
For the same RoBERTa language model with different numbers of transformer layers, 24 layers of transformers had better accuracy than 3 layers,
however, its training cost grows for the larger language model.
For the same fine-tuned language model, classes number has some impact on the accuracy,
the fewer classes the dataset has, the more accurate results the model can achieve.


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|r|}
\hline
Model & Dimensions & Epochs & Accuracy \\
\hline\hline
ResNet   & 18 & 50	&	56.25\% \\
ResBertNet  & 18res+3trans & 10	&	80.00\% \\
\hline
\end{tabular}
\end{center}
\caption{Comparision of accuracy of character detection models on the Baidu Street View 2019 dataset.}
\end{table}


\subsection{Discussion}

 Compare to the accuracy of 56.4\% on the Baidu-Street-2019 dataset, our models have preferable better results.
The reason that the model's outstanding performance lies in the large pre-trained data for BERT, on top of that fine-tuned by RoBERTa, and use of extra language corpus of
$5.4$ billions of tokens of wiki data and other resources.
The transformers in the pre-trained language models use multiple layers for representing images and may be used in other Computer Vision task, \eg object detection, gesture prediction.


\section{Conclusion}

This paper proposed a novel idea by using pre-trained language models for scene Chinese text detection and recognition.
The finding improve the accuracy in OCR for Chinese street scene, and might improve the accuracy of other language scene detection and recognition.
Tests showed that the proposed model outperforms the ResNet model without augmentation on the image dataset,
the model achieved accuracy of $88.74\%$ on the Baidu 2019 street view text recognition set.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{cpvr_detrec}
%\begin{thebibliography}{10}
%
%  \bibitem{lee2009convolutional}
%Lee, H., Grosse, R., Ranganath, R. \&  Ng, A.~Y.
%\newblock {Convolutional deep belief networks for scalable unsupervised
%  learning of hierarchical representations}.
%\newblock In {\em Proceedings of the 26th annual international conference on
%  machine learning}, pages 609--616, 2009.
%
%
%\bibitem{nair2010rectified}
%Nair, V. \&  Hinton, G.~E.
%\newblock {Rectified linear units improve restricted boltzmann machines}.
%\newblock In {\em ICML}, 2010.
%
%
%\bibitem{mikolov2013distributed}
%Mikolov, T., Sutskever, I., Chen, K., Corrado, G.~S. \&  Dean, J.
%\newblock {Distributed representations of words and phrases and their
%  compositionality}.
%\newblock In {\em Advances in neural information processing systems}, pages
%  3111--3119, 2013.
%
%
%\bibitem{devlin2019bert}
%Devlin, J., Chang, M.-W., Lee, K. \&  Toutanova, K.
%\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
%  Understanding}.
%\newblock In {\em NAACL-HLT (1)}, 2019.
%
%
%\bibitem{vincent2008extracting}
%Vincent, P., Larochelle, H., Bengio, Y. \&  Manzagol, P.-A.
%\newblock {Extracting and composing robust features with denoising
%  autoencoders}.
%\newblock In {\em Proceedings of the 25th international conference on Machine
%  learning}, pages 1096--1103, 2008.
%
%
%\bibitem{chen2020generative}
%Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal, P., Luan, D.
% and Sutskever, I.
%\newblock {Generative Pretraining from Pixels}.
%\newblock In {\em Proceedings of the 37th International Conference on Machine
%  Learning}, 2020.
%
%
%\bibitem{wang2019development}
%Wang, W., Yang, Y., Wang, X., Wang, W. \&  Li, J.
%\newblock {Development of convolutional neural network and its application in
%  image classification: a survey}.
%\newblock {\em Optical Engineering}, 58(4):40901, 2019.
%
%
%\bibitem{lecun1990handwritten}
%LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
%  W.~E. \&  Jackel, L.~D.
%\newblock {Handwritten digit recognition with a back-propagation network}.
%\newblock In {\em Advances in neural information processing systems}, pages
%  396--404, 1990.
%
%
%\bibitem{krizhevsky2012imagenet}
%Krizhevsky, A., Sutskever, I. \&  Hinton, G.~E.
%\newblock {Imagenet classification with deep convolutional neural networks}.
%\newblock In {\em Advances in neural information processing systems}, pages
%  1097--1105, 2012.
%
%
%\bibitem{zeiler2014visualizing}
%Zeiler, M.~D. \&  Fergus, R.
%\newblock {Visualizing and understanding convolutional networks}.
%\newblock In {\em European conference on computer vision}, pages 818--833.
%  Springer, 2014.
%
%
%\bibitem{simonyan2014very}
%Simonyan, K. \&  Zisserman, A.
%\newblock {Very deep convolutional networks for large-scale image recognition}.
%\newblock In {\em 3rd International Conference on Learning Representations,
%  ICLR 2015 - Conference Track Proceedings}, San Diego, CA, United states,
%  2015.
%
%
%\bibitem{szegedy2015going}
%Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
%  Vanhoucke, V. \&  Rabinovich, A.
%\newblock {Going deeper with convolutions}.
%\newblock In {\em Proceedings of the IEEE conference on computer vision and
%  pattern recognition}, pages 1--9, 2015.
%
%
%\bibitem{he2016deep}
%He, K., Zhang, X., Ren, S. \&  Sun, J.
%\newblock {Deep residual learning for image recognition}.
%\newblock In {\em Proceedings of the IEEE conference on computer vision and
%  pattern recognition}, pages 770--778, 2016.
%
%
%\bibitem{xie2017aggregated}
%Xie, S., Girshick, R., Doll{\'{a}}r, P., Tu, Z. \&  He, K.
%\newblock {Aggregated residual transformations for deep neural networks}.
%\newblock In {\em Proceedings of the IEEE conference on computer vision and
%  pattern recognition}, pages 1492--1500, 2017.
%
%
%
%\bibitem{szegedy2016inception}
%Szegedy, C., Ioffe, S., Vanhoucke, V. \&  Alemi, A.~A.
%\newblock {Inception-v4, inception-ResNet and the impact of residual
%  connections on learning}.
%\newblock In {\em 31st AAAI Conference on Artificial Intelligence, AAAI 2017},
%  pages 4278--4284, San Francisco, CA, United states, 2017.
%
%
%\bibitem{huang2016deep}
%Huang, G., Sun, Y., Liu, Z., Sedra, D. \&  Weinberger, K.~Q.
%\newblock {Deep networks with stochastic depth}.
%\newblock In {\em European conference on computer vision}, pages 646--661.
%  Springer, 2016.
%
%
%
%\bibitem{lu2007survey}
%Lu, D. \&  Weng, Q.
%\newblock {A survey of image classification methods and techniques for
%  improving classification performance}.
%\newblock {\em International journal of Remote sensing}, 28(5):823--870, 2007.
%
%
%\bibitem{cui2020revisiting}
%Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. \&  Hu, G.
%\newblock {Revisiting Pre-Trained Models for Chinese Natural Language
%  Processing}.
%\newblock In {\em Findings of EMNLP}. Association for Computational
%  Linguistics, 2020.
%
%
%\bibitem{he2017fine}
%He, X. \&  Peng, Y.
%\newblock {Fine-grained image classification via combining vision and
%  language}.
%\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
%  Pattern Recognition}, pages 5994--6002, 2017.
%
%
%\bibitem{reed2016learning}
%Reed, S., Akata, Z., Lee, H. \&  Schiele, B.
%\newblock {Learning deep representations of fine-grained visual descriptions}.
%\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
%  Pattern Recognition}, pages 49--58, 2016.
%
%
%\bibitem{fernandez2014we}
%Fern{\'{a}}ndez-Delgado, M., Cernadas, E., Barro, S. \&  Amorim, D.
%  Amorim.
%\newblock {Do we need hundreds of classifiers to solve real world
%  classification problems?}
%\newblock {\em The journal of machine learning research}, 15(1):3133--3181,
%  2014.
%
%
%
%\bibitem{kim2014convolutional}
%Kim, Y.
%\newblock {Convolutional neural networks for sentence classification}.
%\newblock In {\em EMNLP 2014 - 2014 Conference on Empirical Methods in Natural
%  Language Processing, Proceedings of the Conference}, pages 1746--1751, Doha,
%  Qatar, 2014.
%
%
%
%\end{thebibliography}
}

\end{document}
