% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
% Include other packages here, before hyperref.


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{7435} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Unify Language and Vision: An Efficient COVID-19 Tomography Image Classification Approach}

\author{Dezhou Shen\\
Department of Computer Science\\
Tsinghua University\\
Beijing, CN 100084\\
{\tt\small sdz15@mails.tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}

  An accurate and efficient image classification algorithm used in the COVID-19 detection from the lung computed tomography
  can be of great help for the doctors working in places without advance equipments of diagnostics.
  The machine with high accuracy COVID-19 classification model can relief the burden by making testing and checking
  thousands of people's tomography images easier in a specific region, which is suffering from the COVID-19 outbreak incidents.
  By encoding image pixels and metadata using the pre-trained language models of
  \underline{B}idirectional \underline{E}ncoder \underline{R}epresentations from \underline{T}ransformers,
  then connect to a fully connected layer,
  the classification model outperforms the ResNet model and the DenseNet image classification model,
  and achieved accuracy of $99.51\%\sim100.00\%$ on the COVID-19 tomography image test set.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

  Natural language processing and Computer Vision are two important domains in the Computer Science.
  However, the encoded representation for the languages and images showed that there are potential connections
  between what we see and what we write.
  Most research have laid lack of attention on the relevance between the natural language processing and the Computer Vision.
  In this paper, I discussed a novel method by unifying the representation of languages and visions from the representation layer.
  Unsupervised pre-training is important to the modern research in deep learning.
  Lee \etal~\cite{lee2009convolutional} used the pre-training approaches in Computer Vision tasks in 2009, and later from 2010 to 2016,
  Nair and Hinton~\cite{nair2010rectified} proved that the pre-training process is supplementary in the Computer Vision tasks,
  thus, can be omitted in some cases.
  However, it started to flourish in the natural language processing domain since Mikolov \etal~\cite{mikolov2013distributed} had proposed Word2Vec.
  Not long after, Devin \etal~\cite{devlin2019bert}'s Bidirectional Encoder Representations from Transformers language model dominates
  in most frequently used tasks in the natural language processing domain, which is close resemble of Vicent \etal~\cite{vincent2008extracting}'s denoising autoencoder model, which was initially designed for images.
  Therefore, the pre-training process becomes one of the most important procedures in deep learning.

%-------------------------------------------------------------------------

\section{Recent Work}

  Chen \etal~\cite{chen2020generative} trained image representation by sequence Transformers and tested on the CIFAR-10 dataset to show it is outperforming to Wide-ResNet,
which was inspired by unsupervised natural language representation learning.
  Wang \etal~\cite{wang2019development} reviewed that convolutional neural networks had been proposed in the 1960s, and had its implementation in the 1980s,
and until LeCun \etal~\cite{lecun1990handwritten}'s first experiment on handwritten digit recognition, CNN's great potential had been revealed.
  In the 2010s, Krizhevsky \etal~\cite{krizhevsky2012imagenet} proposed the deep architecture, AlexNet, by concatenating multiple components of CNN layers.
Several years later, a lot of variants of AlexNet had been proposed by researchers and the accuracy of ImageNet had been greatly improved, \eg ZFNet~\cite{zeiler2014visualizing}, VGG~\cite{simonyan2014very}, GoogLeNet~\cite{szegedy2015going}, ResNet~\cite{he2016deep},
 ResNeXt~\cite{xie2017aggregated}, inception-ResNet-v2~\cite{szegedy2016inception}, DenseNet~\cite{huang2016deep}.
  Lu and Weng~\cite{lu2007survey} concluded that for the multi-source image classification tasks, additional information such as signatures, texture, context, and ancillary data can be combined to achieve better performance.
And it is difficult to handle the dichotomy between pixels and natural language texts in a single model.
  Cui \etal~\cite{cui2020revisiting} proposed several whole-word-masking pre-trained Chinese language models,
which are improved versions of BERT~\cite{devlin2019bert}, one of the popular pre-trained language models, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large.
These models achieved better performance in Chinese machine reading comprehension, Chinese document classification, and other downstream natural language tasks.
  He and Peng~\cite{he2017fine} combined the vision stream and language stream as two parallel channels for extracting multi-source information in the image classification task,
and tested on the CUB-200-2011 image dataset and achieved $85.55\%$ by combining GoogLeNet~\cite{szegedy2015going} and CNN-RNN~\cite{reed2016learning}, the result outperformed many competitors.


%-------------------------------------------------------------------------

\section{Image Classification via Pre-trained Transformers Language Models}

  Three of the most popular approaches for image classification tasks are per-pixel, subpixel, and heterogeneous.
Lu and Weng found that, for per-pixel approach, non-parametric classifiers, \eg neural networks, support vector machines, and decision trees,
are the most well-known algorithms for their performance and generalization advantages in the late 1990s and 2000s.
Fern{\'a}ndez \etal~\cite{fernandez2014we} compared different classifiers in small datasets, and they found that the random forest algorithm ranks first among the 179 classifiers.

%-------------------------------------------------------------------------
\subsection{Dataset}

In total, 349 computed tomography images were collected from the COVID-19 reports in the MedPix, LUNA, Radiopaedia,
PubMed Central databases, which sizes range from $1637\times1225$ to $148\times61$.


\subsection{Approach}

My approach consists of one pre-training stage followed by one fine-tuning stage.
In pre-training stage, I use the BERT objectives and the sequence Transformer architecture to predict the language tokens.

\par Given an unlabeled dataset $\Bbb X$, the BERT objective samples a sub-sequence $\Bbb S \in \{C\}$,
$C$ represents all possible tokens, and such that for each index $i \in \Bbb S$,
there is independent probability $15\%$ of appearing in $\Bbb S$,
name the token $M$ as the BERT mask.
As equation (1), train the language model by minimizing the BERT objective of the "masked" elements $x_M$
conditioned on the "unmasked" ones $x_{\left[1,n\right]\backslash M}$.

\begin{equation}
  \lambda = \mathop{\Bbb E}\limits_{x\sim\Bbb X} \mathop{\Bbb E}\limits_{M} \sum_{i\in S}{\bigl[-\log{p(x_i\mid x_{\left[1,n \right]\backslash M})\bigr]}}
\end{equation}

\par The transformer decoder takes the image pixels and meta characters sequence $x_1,\cdots,x_n$ and produces a $d$-dimensional
embedding for each position.
Then I use a fully connected layer as a non-linear function mapping from the embeddings to the image classes.
A dropout layer and a Softmax layer are used for better transfer performance between the training dataset and the test dataset.

\subsection{Per-pixel Encoder}

In the per-pixel image classification approach, for every RGB channel of pixels in an image,
each pixel has its pixel-channel code, ranging from $0x00$ to $0xff$ for different colors.
Thus, these pixels of an image are resemble to ASCII characters in a document.
Generality speaking, the performance of the pre-trained language models achieved in the document classification tasks,
can be transferred to image classification naturally.

\par Recall that Kim~\cite{kim2014convolutional} had proved that unsupervised pre-trained language model $word2vec$ and CNN outperformed many other machine learning algorithms,
\eg support vector machines and conditional random field, in many datasets such as movie reviews, Stanford sentiment treebank, and the \underline{T}ext \underline{RE}trieval \underline{C}onference question.
Cui \etal's pre-trained language model, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large, had improved performances over many other machine learning algorithms.
BERT and RoBERTa-wwm-ext-large models both achieved an f1-score of 97.8\% in the THUCNews dataset, which contains 65 thousands of news in 10 domains.

\par From Table (1), by combining the pre-trained language model with the fully connected layer as the document classification model,
the test accuracy exceeds the other popular machine learning algorithms.
Therefore, the pixel channels of an image can be properly represented by these pre-trained language models.

\subsection{Model Architecture}

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{classification.pdf}}
\end{center}
   \caption{Concatenation, encoder, representation, and extraction layers for image classification task.}
\label{fig:short}
\end{figure*}

I design simple classification models without too many layers, as Figure (1) shows.
And use the COVID-19 lung tomography dataset as an example to show the architecture of the model.
The model architecture has seven functional layers:

\begin{itemize}
\item {\bf Input layer}
\item {\bf Concatenation layer}
\item {\bf Trim layer}
\item {\bf Encoder layer}
\item {\bf Embedding layer}
\item {\bf Feature Extraction layer}
\item {\bf Output layer}
\end{itemize}

\par The original tomography images are of different sizes and often too large to feed into the model.

The COVID-19 lung tomography dataset contains 251 covid images and 291 covid negative images in the training set,
and contains 98 covid image and 100 covid negative images in the test set.
All images are resized to a resolution of $32\times32$ pixels, the RGB channels are kept and the alpha channel values are discarded.
Encode the image by the sequence of RGB channel values, in the order of Red-channel, Green-channel, and Blue-channel,
then encode other metadata which is the file name of each image, as a sequence of ASCII characters.
In the Concatenation layer, the pixel-channel value and metadata are concatenated, put a special token of $\textbf{[CLS]}$ at the start,
and put a $\textbf{[SEP]}$ token between channel values and metadata, put a special token of $\textbf{[SEP]}$ at the end.
In the Trim layer, due to the limit on the max sequence of the BERT language model, a sequence larger than $512$ needs to be trimmed before sending it to the BERT model.
Keep the first 256 characters and last 256 characters of the concatenated sequence, trimmed result contains the first 255 red-channel value,
some blue-channel value, and all the meta value for common cases.
In the Encoder layer and the Embedding layer, trimmed sequence of values are encoded by BERT-like models,
and get the encoded representation of the token $\textbf{[CLS]}$ as the images' language model embeddings.
In the Feature Extraction layer, the combination of a dropout layer, a fully connected layer, and a softmax layer, as equation (2), can provide
transfer ability for the model from the training dataset to the test dataset.
In the Output layer, the classification label of the image is feed into the model.

\begin{equation}
  \sigma(\textbf{z})_j = {e^{\textbf{z}_j} \over {\sum_{k=1}^{\textit{K}} e^{\textbf{z}_k}}}, j=1,\cdots,\textit{K}.
\end{equation}

\subsection{Fair Comparison for Test: The Mask Token}

\par It is intuitive for the model to use both the channel values and the metadata during the training phase.
For the COVID-19 lung tomography dataset, the metadata is the filename of the images, which can be treated as the supervised description of the image.
Peek some the filenames, \eg \textit{covid\_201\_1.png}, \textit{covid\_201\_2.png}, and \textit{noncovid\_001\_1.png},
and it is beneficial for the classification model to understand the supervised descriptive information for the image.

\par Recall that the $\textbf{[MASK]}$ token in the BERT language model is a special token which can be used to trim the vocabulary size,
and gives the token-recovery capability to the downstream task model.

\par However, in the test phase, if feeding the metadata to the model, other people might argue that the model learns only the mapping function from
the metadata, which is the filename in this case, to the image class
which makes it unfair to compare the performance with other image classification models.
Thus, to be more objective and fair, I add a preprocess for the metadata in the test phase, using the $\textbf{[MASK]}$ token to replace each
ASCII character of the filename then feeds into the model to do the inference.

\par The $\textbf{[MASK]}$ token provide a fair comparison during the test by masking the language features, therefore,
it can be seen that the language and the vision fusions during the training and enhance the performance and accuracy for the classification model
of the CIFAR dataset and the COVID-19 tomography dataset.

%-------------------------------------------------------------------------


\section{Experiments and Results}


\subsection{Training}

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{image-classification.pdf}}
\end{center}
   \caption{Accuracy of the image classification models with the pre-trained language encoder on the CIFAR-10, CIFAR-100, and COVID-19 test dataset.}
\label{fig:short}
\end{figure*}


As Cui \etal~\cite{cui2020revisiting} reported that the training of the RBT3 language model was based on Devlin \etal's model,
moreover, the pre-trained Chinese language models use extra 5 billion training tokens comparing to Devlin \etal's 0.4 billion tokens.
I use a batch size of 512 and train for $6\sim16$ epochs using AdamW with $\beta_1=0.99$, $\beta_2=0.999$, and weight decay of $10^{-8}$.
I set the learning rate to $10^{-4}$ and no warmed up or decay are used.
The dropout rate is $0.05$.

\par For the fine-tuning stage, as Figure (1), I check the accuracy of the trained model on the test set and stopped training when it converges on $10^{-3}\sim10^{-4}$.

\par The experiment was performed on a Google Cloud TPU v3, with 32GB of RAM, and 8 chips with 16GB of the high speed of memory each,
which can provide 420 tera-flops of computation capability.

\subsection{Results}

Use the proposed model, and I tried different pre-trained language models to see the impact on classification accuracy.
From the Table (1), it can be seen that the models are trained on the training dataset and validated on the test dataset.
For the same size of a dataset with larger classes, it needs more epochs and training time for the classification model,
and the training epochs ranges from 3 to 8.
For the same RoBERTa language model with different numbers of transformer layers, 24 layers of transformers had better accuracy than 3 layers,
however, its training cost grows for the larger language model.
For the same fine-tuned language model, classes number has some impact on the accuracy,
the fewer classes the dataset has, the more accurate results the model can achieve.


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|r|}
\hline
LM& Dimensions & Epochs & Accuracy \\
\hline\hline
RBT3   & 768 & 8	&	99.51\% \\
RBTL3  & 1024 & 3	&	100.00\% \\
RoBERTa-large  & 1024  & 10	&	100.00\% \\
\hline
\end{tabular}
\end{center}
\caption{Comparision of accuracy of the pre-trained language image classification models on the COVID-19 dataset.
RoBERTa-large is short for RoBERTa-wwm-ext-large. LM is short for Language Model}
\end{table}


\subsection{Discussion}

 Compare to iGPT-L's accuracy of 96.3\% on the CIFAR-10 dataset without augmentation and 82.8\% on the CIFAR-100 dataset, our models have preferable better results.
The reason that the model's outstanding performance lies in the large pre-trained data for BERT, on top of that fine-tuned by RoBERTa, and use of extra language corpus of
$5.4$ billions of tokens of wiki data and other resources.
The transformers in the pre-trained language models use multiple layers for representing images and may be used in other Computer Vision task, \eg object detection, gesture prediction.


\section{Conclusion}

This paper proposed a novel idea by using pre-trained language models for image representation and take image classification as an example of its performance in Computer Vision tasks.
The finding might benefit the diagnostics of the COVID-19, and improve the accuracy by combining language pre-trained model and the metadata of the computed tomography.
Tests showed that the proposed model outperforms the iGPT-L model without augmentation on the image dataset,
the model achieved accuracy of $99.60\%\sim99.74\%$ on the CIFAR-10 image set,
and accuracy of $99.10\%\sim99.76\%$ on the CIFAR-100 image set.
Tested on the COVID-19 tomography image test set, the proposed model achieved accuracy of $99.51\%\sim100.00\%$.


{\small
\bibliographystyle{ieee_fullname}
%\bibliography{cpvr_classification}
\begin{thebibliography}{10}

  \bibitem{lee2009convolutional}
Lee, H., Grosse, R., Ranganath, R. \&  Ng, A.~Y.
\newblock {Convolutional deep belief networks for scalable unsupervised
  learning of hierarchical representations}.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 609--616, 2009.


\bibitem{nair2010rectified}
Nair, V. \&  Hinton, G.~E.
\newblock {Rectified linear units improve restricted boltzmann machines}.
\newblock In {\em ICML}, 2010.


\bibitem{mikolov2013distributed}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.~S. \&  Dean, J.
\newblock {Distributed representations of words and phrases and their
  compositionality}.
\newblock In {\em Advances in neural information processing systems}, pages
  3111--3119, 2013.


\bibitem{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K. \&  Toutanova, K.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding}.
\newblock In {\em NAACL-HLT (1)}, 2019.


\bibitem{vincent2008extracting}
Vincent, P., Larochelle, H., Bengio, Y. \&  Manzagol, P.-A.
\newblock {Extracting and composing robust features with denoising
  autoencoders}.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 1096--1103, 2008.


\bibitem{chen2020generative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal, P., Luan, D.
 and Sutskever, I.
\newblock {Generative Pretraining from Pixels}.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.


\bibitem{wang2019development}
Wang, W., Yang, Y., Wang, X., Wang, W. \&  Li, J.
\newblock {Development of convolutional neural network and its application in
  image classification: a survey}.
\newblock {\em Optical Engineering}, 58(4):40901, 2019.


\bibitem{lecun1990handwritten}
LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
  W.~E. \&  Jackel, L.~D.
\newblock {Handwritten digit recognition with a back-propagation network}.
\newblock In {\em Advances in neural information processing systems}, pages
  396--404, 1990.


\bibitem{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I. \&  Hinton, G.~E.
\newblock {Imagenet classification with deep convolutional neural networks}.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.


\bibitem{zeiler2014visualizing}
Zeiler, M.~D. \&  Fergus, R.
\newblock {Visualizing and understanding convolutional networks}.
\newblock In {\em European conference on computer vision}, pages 818--833.
  Springer, 2014.


\bibitem{simonyan2014very}
Simonyan, K. \&  Zisserman, A.
\newblock {Very deep convolutional networks for large-scale image recognition}.
\newblock In {\em 3rd International Conference on Learning Representations,
  ICLR 2015 - Conference Track Proceedings}, San Diego, CA, United states,
  2015.


\bibitem{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V. \&  Rabinovich, A.
\newblock {Going deeper with convolutions}.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.


\bibitem{he2016deep}
He, K., Zhang, X., Ren, S. \&  Sun, J.
\newblock {Deep residual learning for image recognition}.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.


\bibitem{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'{a}}r, P., Tu, Z. \&  He, K.
\newblock {Aggregated residual transformations for deep neural networks}.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.



\bibitem{szegedy2016inception}
Szegedy, C., Ioffe, S., Vanhoucke, V. \&  Alemi, A.~A.
\newblock {Inception-v4, inception-ResNet and the impact of residual
  connections on learning}.
\newblock In {\em 31st AAAI Conference on Artificial Intelligence, AAAI 2017},
  pages 4278--4284, San Francisco, CA, United states, 2017.


\bibitem{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D. \&  Weinberger, K.~Q.
\newblock {Deep networks with stochastic depth}.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.



\bibitem{lu2007survey}
Lu, D. \&  Weng, Q.
\newblock {A survey of image classification methods and techniques for
  improving classification performance}.
\newblock {\em International journal of Remote sensing}, 28(5):823--870, 2007.


\bibitem{cui2020revisiting}
Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. \&  Hu, G.
\newblock {Revisiting Pre-Trained Models for Chinese Natural Language
  Processing}.
\newblock In {\em Findings of EMNLP}. Association for Computational
  Linguistics, 2020.


\bibitem{he2017fine}
He, X. \&  Peng, Y.
\newblock {Fine-grained image classification via combining vision and
  language}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5994--6002, 2017.


\bibitem{reed2016learning}
Reed, S., Akata, Z., Lee, H. \&  Schiele, B.
\newblock {Learning deep representations of fine-grained visual descriptions}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 49--58, 2016.


\bibitem{fernandez2014we}
Fern{\'{a}}ndez-Delgado, M., Cernadas, E., Barro, S. \&  Amorim, D.
  Amorim.
\newblock {Do we need hundreds of classifiers to solve real world
  classification problems?}
\newblock {\em The journal of machine learning research}, 15(1):3133--3181,
  2014.



\bibitem{kim2014convolutional}
Kim, Y.
\newblock {Convolutional neural networks for sentence classification}.
\newblock In {\em EMNLP 2014 - 2014 Conference on Empirical Methods in Natural
  Language Processing, Proceedings of the Conference}, pages 1746--1751, Doha,
  Qatar, 2014.



\end{thebibliography}
}

\end{document}
