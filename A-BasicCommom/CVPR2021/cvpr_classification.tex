% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
% Include other packages here, before hyperref.


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{7777} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Enhance Image Classification Performance Via Unsupervised Pre-trained Transformers Language Models}

\author{Dezhou Shen\\
Department of Computer Science\\
Tsinghua University\\
Beijing, CN 100084\\
{\tt\small sdz15@mails.tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}

  Image classification and categorization are essential for the capability of telling the difference between images for a machine.
  As \underline{B}idirectional \underline{E}ncoder \underline{R}epresentations from \underline{T}ransformers became popular in many tasks of natural language processing recent years,
  it is intuitive to use these pre-trained language models for enhancing the computer vision tasks, \eg image classification.
  In this paper, by encoding image pixels using pre-trained transformers, then connect to a fully connected layer,
  the classification model outperforms the Wide ResNet model and the linear-probe iGPT-L model,
  and achieved an accuracy of $97.72\%~99.60\%$ on the CIFAR-10 image set, and an accuracy of $98.27\%~98.95\%$ on the CIFAR-100 image set.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

  Unsupervised pre-training is important in modern research of deep learning.
  Lee \etal~\cite{lee2009convolutional} used pre-training approaches
  in Computer Vision tasks in 2009, and later from 2010 to 2016, Nair and Hinton~\cite{nair2010rectified} proved that the pre-training process is supplementary in the CV tasks,
  thus, can be omitted in some cases.
  However, it started to flourish in natural language processing domain since Mikolov \etal~\cite{mikolov2013distributed} had proposed Word2Vec.
  Not long before Devin \etal~\cite{devlin2018bert}'s BERT language model dominates in most frequent used tasks in natural language processing,
  which is close resemble of Vicent \etal~\cite{vincent2008extracting}'s Denoising Autoencoder model, which was initially designed for images.
  Pre-training process become one of the most important procedures in deep learning.

%-------------------------------------------------------------------------

\section{Recent Work}

  Chen \etal~\cite{chen2020generative} trained image representation by sequence Transformers and tested on CIFAR-10 to show its outperforming to Wide-ResNet
which inspired from unsupervised natural language representation learning.
  Wang \etal~\cite{wang2019development} reviewed that convolutional neural networks had been proposed in 1960s, and had its implementation in 1980s,
and until LeCun \etal~\cite{lecun1990handwritten}'s first experiment on handwritten digit recognition, CNN's great potential had been revealed.
  In the 2010s, Krizhevsky \etal~\cite{krizhevsky2012imagenet} proposed the deep architecture, AlexNet, by concatenating multiple components of CNN layers.
Several years later, a lot of variants of AlexNet had been proposed by researchers and the accuracy of ImageNet had been greatly improved, \eg ZFNet~\cite{zeiler2014visualizing}, VGG~\cite{simonyan2014very}, GoogLeNet~\cite{szegedy2015going}, ResNet~\cite{he2016deep},
Wide-ResNet~\cite{zagoruyko2016wide}, ResNeXt~\cite{xie2017aggregated}, inception-ResNet-v2~\cite{szegedy2016inception}, DenseNet~\cite{huang2016deep}.
  Lu and Weng~\cite{lu2007survey} concluded that for multi-source image classification task, additional information such as signatures, texture, context and ancillary data can be combined to achieve better performance.
And it is difficult in handling the dichotomy between pixels and natural language texts in a single model.
  Cui \etal~\cite{cui2019pre, cui2020revisiting} proposed several whole-word-masking pre-trained Chinese language models,
which are improved versions of BERT~\cite{devlin2018bert} and RoBERTa~\cite{liu2019roberta} pre-trained language models, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large.
These models achieved better performance in Chinese machine reading comprehension, Chinese document classification and other downstream natural language tasks.

%-------------------------------------------------------------------------

\section{Image Classification via Pre-trained Transformers Language Models}

Three of the most popular approaches for image classification task are per-pixel, subpixel, and heterogeneous.
Lu and Weng found that, for per-pixel approach, non-parametric classifiers, \eg neural networks, support vector machines and decision trees,
are most well known algorithms for their performance and generalization advantages in late 1990s and 2000s.
Fern{\'a}ndez \etal~\cite{fernandez2014we} compared different classifiers in small datasets, and they found that the random forest algorithm ranks first among the 179 classifiers.

%-------------------------------------------------------------------------
\section{Per-pixel Encoder}

For the per-pixel image classification approach, for every RGB channel of pixel in a image,
each pixel had its pixel-channel code, ranges from $0x00$ to $0xff$ for different colors.
Thus, taking these pixels in a image is identical to ASCII characters in a document.
Generality speaking, the performance of the pre-trained language models achieved in document classification task, can be transferred to image classification naturally.

\par Recall that Kim~\cite{kim2014convolutional} had proved that unsupervised pre-trained language model $word2vec$ and CNN outperformed many other machine learning algorithms,
\eg support vector machines and conditional random field, in many dataset such as, movie reviews, Stanford sentiment treebank, and TREC question.
Adhikari \etal~\cite{adhikari2019docbert} showed the BERT and BERT-large model had achieved higher accuracy in the Routers, IMDB, and Yelp dataset which classification ranges from 5 to 90.
Cui \etal's pre-trained language model, namely RBT3, RBTL3, and RoBERTa-wwm-ext-large, had improved performances over many other machine learning algorithms.
BERT and RoBERTa-wwm-ext-large models both achieved an f1-score of 97.8\% in the THUCNews dataset, which contains 65 thousands of news in 10 domains.

\par I used RBTL3 and RoBERTa-wwm-ext-large pre-trained language model in two Chinese Judicial dataset, which have 2 and 4 main domains.
The $Case-2$ dataset annotated civil and criminal cases, and the $Case-4$ dataset annotated by civil, criminal, intellectual property, and administrative cases.
The $Case-2$ dataset has $19508$ training documents and $2000$ test documents, and the $Case-4$ dataset has $34789$ training documents and $2013$ test documents.

\par From table-1, by combining pre-trained language model with fully connected layer as the document classification model,
the test accuracy exceeds the other popular machine learning algorithms.
Therefore, the pixel channels of image can be properly represented by these pre-trained language models.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|r|}
\hline
Language Model 	& Dataset & Accuracy \\
\hline\hline
RoBERTa-wwm-ext-large & Case-2	&	99.8\% \\
RoBERTa-wwm-ext-large & Case-4	&	98.95\% \\
RBTL3  & Case-4	&	96.35\% \\
\hline
\end{tabular}
\end{center}
\caption{Comparision of accuracy of the pre-trained language models on different dataset in the document classification task.}
\end{table}
%-------------------------------------------------------------------------


\section{Image Classification Model}

\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{classification.pdf}}
\end{center}
   \caption{Concat, encoder, representation, and extraction layers for image classification task.}
\label{fig:short}
\end{figure*}

We design a simple classification models without too many layers, as Figure-1 shows.
And use the CIFAR-10 and CIFAR-100 dataset as an example to show the architecture of the model.
The model architecture has seven functional layers:
\begin{itemize}
\item {\bf Input layer}
\item {\bf Concat layer}
\item {\bf Trim layer}
\item {\bf Encoder layer}
\item {\bf Embedding layer}
\item {\bf Feature Extraction layer}
\item {\bf Output layer}
\end{itemize}

\par The CIFAR-10 dataset contains 60000 colour images with a resolution of 32x32 in 10 classes, and the CIFAR-100 dataset has 100 classes containing 600 images each.
Encode the image by the sequence of RGB channel values, in the order of Red-channel, Green-channel, and Blue-channel,
then encode other meta-data if provided, as a sequence of ASCII character.
In the Concat layer, the pixel-channel value and meta data are concatenated, put a special token of $\textbf{[CLS]}$ at the start,
and put a $\textbf{[SEP]}$ token between channel vules and meta values, put a special token of $\textbf{[SEP]}$ at the end.
In the Trim layer, due to the limit on the max sequence of BERT language model, sequence larger than $512$ need to be trimmed before sending to BERT model.
Keep the first 256 characters and last 256 characters of the concatenated sequence, trimmed result contrains the first 255 red-channel value,
some blue-channel value, and all the meta value in common cases.
In the Encoder layer and the Embedding layer, trimmed sequence of values are encoded by BERT-like models,
and get the encoded representation of the token $\textbf{[CLS]}$ as the images' language model embeddings.
In the Feature-Extraction layer, a combination of one dropout layer, one fully connected layer, and one softmax layer is used.
In the Output layer, the classification label of the image is feed in the model.

%-------------------------------------------------------------------------


\section{Image Classification Model}

\subsection{Experiments and Results}

Use the proposed model, and we tried different pre-trained language models to see the impact on classification accuracy.
From the table-2, from line 2 and line 4, for the same size of dataset with larger classes, it need more epochs and training time for the classification model.
From line 2 and line 3, for the same RoBERTa language model with different numbers of transformer layers, 24 layers of transformers had better accuracy than 3 layers,
however, its training cost grows for the larger language model.
From line 3 and line 5, for the same fine-tuned language model, classes number has some impact on the accuracy,
the less classes the dataset has, the more accuracy the model can achieve.


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|r|}
\hline
Language Model 	& Dataset & Epoch & Accuracy \\
\hline\hline
RBT3   & CIFAR-10 & 1	&	84.89\% \\
RBTL3  & CIFAR-10 & 1	&	97.72\% \\
RoBERTa-wwm-ext-large  & CIFAR-10 & 6	&	99.60\% \\
RBTL3  & CIFAR-100 & 6  & 	98.27\% \\
RoBERTa-wwm-ext-large & CIFAR-100 & 16 &	98.95\% \\
\hline
\end{tabular}
\end{center}
\caption{Comparision of accuracy of the pre-trained language models on the CIFAR-10 and CIFAR-100 datasets in the image classification task.}
\end{table}


\subsection{Discussion}

 Compare to iGPT-L's accuracy of 96.3\% on the unaugmented CIFAR-10 dataset and 82.8\% on the CIFAR-100 dataset, our models has preferable better results.
The reason that the model's outstanding performance lie in the large pre-trained data for BERT, on top of that fine-tuned by RoBERTa, and use extra language corpus of
$5.4$ billions of tokens of wiki data and other resources.
The transformers in the pre-trained language models use multiple layers for representing images, and maybe used in other Computer Vision task, \eg object detection, gesture prediction.


\section{Conclusion}

This paper proposed a novel idea by using pre-trained language models for images representation, and take image classification as an exmaple for its performance.
Tests showed that the proposed model outperforms the iGPT-L model without augmentment on the image dataset,
the model achieved accuracy of $97.72\%~99.60\%$ on the CIFAR-10 image set,
and accuracy of $98.27\%~98.95\%$ on the CIFAR-100 image set.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{cpvr_classification}
}

\end{document}
