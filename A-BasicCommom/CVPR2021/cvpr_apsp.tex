% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
% Include other packages here, before hyperref.


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{297} % \times \times \times  Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Lower Bounds on Rate of Convergence of Matrix Products in All-Pairs Shortest Path of Social Network}

\author{Dezhou Shen\\
Department of Computer Science\\
Tsinghua University\\
Beijing, CN 100084\\
{\tt\small sdz15@mails.tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
  For the calculation of the all-pairs nodes shortest path in networks,
  Alon \etal proposed an efficient algorithm with matrix multiplication.
  In this paper, with the distance product association law and block matrix multiplication,
  the all-pairs shortest path algorithm has lower time-bound of $O(14n^3)$.
  The proposed algorithm leverages the following improvements:
  matrix sparseness judgment, convergence judgment, distance product by matrix multiplication, association law for distance product,
  scale-free characteristics of the social networks degrees and the precision limit of floating point operations on the modern hardware.
  Comparing to Alon \etal's algorithm, the algorithm has improvement of 39\% on CPU and improvement of 36.2 times on GPU
  on the Chinese Movie Celebrity Network $2011\sim2015$ dataset.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}


In recent years, driven by the Asian film industries, such as China and India, the global box office has maintained a steady growth trend.
With the rapid development of social network applications, more and more actors use social networks for work communication.
Therefore, the social network has become an important tool in film distribution, marketing, promotion, and an important medium for film actors and fans to interact.
Previous studies are lack of research on actors' social networks, PageRank, a widely used web page influence algorithm, is not accurate in evaluating social network influence.
Social network feature measurement is an important task in social network data mining, some network characteristics, such as betweenness and closeness, need to calculate the distances between nodes.
As a result, efficient calculation of the shortest path is one of the key problems in social network measurement.
There was Floyd-Warshall~\cite{floyd1962algorithm,warshall1962theorem} algorithm for calculating node distances, Aho and Hopcroft~\cite{aho1974design} proved that distance product is homogenous to matrix multiplication,
Alon \etal~\cite{alon1997exponent} proposed an algorithm to calculate distance product by matrix multiplication, Zwick~\cite{zwick2002all} optimized distance product using fast matrix multiplication,
and Garbow and Winograd~\cite{garbow1985scaling} proposed an algorithm for distance product using matrix multiplication with element bound.
From time-bound of matrix multiplication, Strassen~\cite{strassen1969gaussian}, Coppersmith and Winograd~\cite{coppersmith1987matrix}, Davie and Andrew~\cite{davie2013improved}, Le Gall~\cite{le2014powers} proposed the block matrix multiplication with time-bound of \(O(n ^ {2.807})\), \(O(n^{2.376})\), \(O(n^{2.374})\), \(O(n^{2.373})\).
Lam \etal~\cite{lam1991cache} proposed that the progressive time complexity of matrix multiplication with a cache block size of $\Bbb B$ is \(O({2n^3 \over \Bbb B}+{n^2})\),
and Goto and Geijn~\cite{goto2008anatomy} proposed a new library, OpenBLAS, which has a higher cache hit rate and improved time cost in matrix computation on X86 CPU, compared to modern BLAS libraries, such as Intel MKL v8.1.1, ESSL v4.2.0, and ATLAS v3.7.11.
From point of view of matrix sparseness of network adjacent matrix, for sparse matrix multiplication, SciPy and CuPy use SciPy-sparse and CuPy-cuSparse to achieve efficient sparse matrix multiplication on CPU and GPU.
For dense matrix multiplication, NumPy, SciPy, CuPy, and ND4J use OpenBLAS, cuBLAS, and other libraries to achieve efficient block matrix multiplication on CPU and GPU.
Therefore, a feasible method for calculating the shortest path of full node pairs is: represents the network by adjacent matrix, then makes all matrix elements exponential, computes matrix multiplication, and finally takes the logarithm of matrix elements.
This procedure is called a distance product epoch.
Iterate several times to get all-pairs shortest path result.
Further, the analysis time complexity of the distance product calculation and iteration of each epoch.
For time complexity of the distance product, matrix multiplication is the most complex sub-procedures.

\begin{enumerate}
\item For the distance product optimization, Zwick proposed the distance product with block matrix multiplication, therefore choose proper computation device library with optimized cache for block matrix multiplication.
\item For the iteration optimization, Aho and Hopcroft proved distance product is isomorphic to the matrix multiplication, and regard to that the distance product association law, I optimize the iteration process by reusing the distance product result of the epochs.
\end{enumerate}

%-------------------------------------------------------------------------

\section{Dataset and Hardware}

From January 1, 2011, to March 31, 2015, I collected the film actors' social network following relationship, profile information,
and text message on Sina Weibo, one of the most popular social media in China.
In all, 8508 actors, 577,775 edges, and 12,526,114 Sina Weibo posts are collected.
This dataset is named as Chinese Movie Celebrity Network $2011\sim2015$ dataset.
Experiments are made on a server with two Intel Xeon E5-2620 v4 CPUs and one NVIDIA GeForce GTX 1080 Ti GPU, in total 32 cores with a frequency of 2.1Ghz, 128G system memory, and 11G GPU memory in my experiment.


%-------------------------------------------------------------------------

\section{Comparison On Block Matrix Multiplication Implementtation}

On the Chinese Movie Celebrity Network $2011\sim2015$ dataset, this paper implements Strassen block matrix multiplication,
Copper-Winograd block matrix multiplication, and OpenBLAS matrix multiplication.
On the computer hardware, compared the $8508\times8508$ matrix multiplication time cost.
The block matrix multiplication based on Strassen and Coppersmith-Winograd takes 6 seconds for the matrix multiplication,
while the OpenBLAS library takes only 4 seconds.
The performance of optimized cache block matrix multiplication, such as the OpenBLAS matrix computation library,
is higher than Strassen and Copper-Winograd block matrix multiplication algorithm.
Therefore, the distance product algorithm proposed by Zwick with OpenBLAS block matrix multiplication is better than
the distance product algorithm with Strassen and Coppersmith-Winograd.

%-------------------------------------------------------------------------

\section{Time-Bound Of Distance Product Algorithm With Optimized Block Matrix Multiplication}

For a fully connected network, the graph has a maximum diameter of $n-1$, so using the result reuse method in distance product, that is use the last distance product calculation result, the matrix multiplication times are $log_2(n-1)$.
Albert~\cite{albert1999diameter} found that social networks, the Internet, and other complex networks have scale-free characteristics and small-world phenomena, thus network diameter is much smaller than the number of network nodes.
I present the corresponding nodes counts, diameters, and logarithm of diameters in Table 1, from the observation there is Formula (1).
In social networks, the number of nodes $n$, with cache block size $B\times B$, according to the Formula (9-10), the matrix product calculation iteration times does not exceed the number of $\lceil{\log{\Bbb D}}\rceil$, recall that Lam \etal proved that matrix multiplication time-bound is \(O({2n^3 \over \Bbb B}+{n^2})\),
thus Alon \etal algorithm time-bound is \(O({\log{\Bbb D}2n^3\over{ \Bbb B}})\) with cache block size of $\Bbb B$, use the equation (1) result, time-bound is \(O({\log(\log{N})2n^3\over \Bbb B})\).
With the arithmetic limitation on the floating point operation, \eg take a network with nodes number of $N<10^{308}$
as shown in Table 1, see that the shortest path calculation iterations time is no larger than $\log{\Bbb D} \approx \log(\log{N})<7$, that is, the number of calculation iterations does not exceed the magic number $7$,
bringing a stronger time-bound of \(O({14n^3\over {\Bbb B}})\).
In other words, with the floating point operation precision limit on computer hardware and complex networks scale-free characteristics, all-pairs shortest path algorithm time-bound is \(O({14n^3\over \Bbb B})\), for $\Bbb B$ is a constant, thus, for simpler representation, the time-bound is \(O(14n^3)\).


%-------------------------------------------------------------------------


\section{Shortest Path Algorithm With Block Matrix Multiplication}

Definition 1 ($\textbf{Outer Product Matrix Multiplication}$) Given matrices \(\Bbb A \in \Bbb R^{m\times r}\), \(\Bbb B \in \Bbb R^{r\times n}\) and \(\Bbb C \in \Bbb R^{m\times n}\), the outer product is:
\begin{equation}
\Bbb C = \Bbb C  + \Bbb A(:,k)\bullet \Bbb B(k,:) , k \in (1,r)
\end{equation}

Given two vectors \(\textsc x\), \(\textsc y\), and matrix \(\Bbb A\), having \(\Bbb A \in \Bbb R^{m\times n}\), \(\textsc x \in \Bbb R^{m}\) and \(\textsc y \in \Bbb R^{n}\) the outer product \(\bullet\) is defined by:
\begin{equation}
a_{ij} = a_{ij} + x_i \times  y_j, i \in (1,m), j \in (1,n)
\end{equation}

Definition 2 ($\textbf{Distance Product Matrix Multiplication}$) Given matrices \(\Bbb A \in \Bbb R^{m\times r}\), \(\Bbb B \in \Bbb R^{r\times n}\) and \(\Bbb C \in \Bbb R^{m\times n}\), the distance product is:
\begin{equation}
  \Bbb C = \min(\Bbb C, \Bbb A(:,k)\odot \Bbb B(k,:)) , k \in (1,r)
\end{equation}

Given two vectors \(\textsc x\), \(\textsc y\), and matrix \(\Bbb A\), having \(\Bbb A \in \Bbb R^{m\times n}\), \(\textsc x \in \Bbb R^{m}\) and \(\textsc y \in \Bbb R^{n}\) the distance product \(\odot\) is defined by:
\begin{equation}
a_{ij} = \min(a_{ij} , x_i + y_j), i \in (1,m), j \in (1,n)
\end{equation}
%-------------------------------------------------------------------------
\begin{figure*}
\begin{center}
  \fbox{\rule{0pt}{2in} \includegraphics[width=0.9\linewidth]{figure1.pdf}}
\end{center}
   \caption{Visualization for each epoch: start state of adjacent matrix, iteration states, and the result states of the actors' social network.
   (a) initial state before calculation,
   (b) the result matrix of the first epoch calculation,
   (c) the result matrix of the second epoch calculation,
   (d) the third epoch of calculation, which is also the final state, that is the all-pairs nodes shortest path result matrix.}
\label{fig:short}
\end{figure*}


\subsection{Maximum Element Calculation}

Calculate the largest value of the square matrix elements $\widetilde X$,
this maximum value is latter used in the exponential computations.

\subsection{Exponential Calculation}

The precisions of floating point operation on different computer hardware comply with the IEEE standard~\cite{ieee1985ieee},
first of all, transform the input matrix by exponential transformation, to be specific, taking the number of nodes as the base,
taking the difference between the largest value of the elements $\widetilde X$ and the matrix elements as exponential.
For the actors' social network with 8508 nodes, the diameter of the network does not allow to exceed 9.8 and 78.4
in calculating for the precision of 32-bit floating point and 64-bit floating point,
otherwise, the exponential operation on the computer platform shall overflow.

\subsection{Matrix Multiplication}

Different matrix libraries have various speed in the matrix multiplication on the computer hardware, as shown in Table (3),
choose proper device-based optimized matrix multiplication libraries according to different conditions,
such as matrix sparseness, calculation hardware, and programming language.

%-------------------------------------------------------------------------
\subsection{Logarithm Calculation}
Take non-zero elements of the matrix multiplication result to the element logarithm process, that is, the number of network nodes as the base, the element in the matrix multiplication result as the true number, then round down the logarithm result,
after that take difference between twice the largest input matrix element value $\widetilde X$ and the logarithm result as result, such as Formula (6).

\subsection{Distance Product Association Law}

From the iteration of the shortest path calculation, suppose \(\Bbb A,\ \Bbb B,\ \Bbb C\) is the square matrix, according to the association law of distance product, there is a Formula (7).
Matrix multiplication has an association law, and the distance product is isomorphism to matrix multiplication, so simplify the shortest path iteration process as Formula (8).
Since matrix multiplication has an association law and distance product is isomorphism to matrix multiplication, similarly, distance product has an association law.
Under the premise of the shortest path matrix $L^{(n-1)}$ given the adjacent matrix and the $n-1$ edge, calculate the shortest path matrix $L^{(n)}$ with n edges, and extend the shortest path of the $n-1$ edge by edge.
Calculating the shortest path matrix $L^{(n)}$ can be completed as Formula (9).
As can be seen from the nature of the shortest path in the graph, the shortest path from any of two nodes does not exceed $n-1$, so there is Formula (10).

Definition 3 ($\textbf{Result Reuse in Distance Product}$) According to the nature of the Equation (8), the process of calculating the shortest path iteration using the last result of distance product follows in turn, as shown in Formula (11).
The shortest path needs to be calculated $\lceil{log_2(n-1)}\rceil$ times matrix multiplication, from the Formula (10), it can be seen that $(n-1)\leq2^{\lceil{log_2(n-1)}\rceil}$ is the same as the simple shortest path matrix calculation result $L^{(n-1)}$, so there is a Formula (12).
Obviously, the result reuse method saves $n-2-\lceil{log_2(n-1)}\rceil$ times of matrix multiplication.
Observe the shortest path calculation process of the actors' social network, the adjacent matrix is shown in Figure 1-a, and the change of result in each iteration is shown in Figure 1-(b-d), the shortest path result matrix converges gradually, each element stabilizes as iterations go.


\begin{algorithm}
\caption{Lower Bounds Convergence Matrix Products in the All-Pairs Shortest Path}\label{algorithm}
\KwData{Adjacent Matrix $\Bbb W$, Matrix Row count $n$, Network Diameter $\Bbb D$}
\KwResult{The shortest path result $L^{(m)}$}
  $L^{(1)}\leftarrow \Bbb W$\;
  $m\leftarrow 1$\;
  \While{$m\leq \Bbb D$}{
    $\Bbb A\leftarrow L^{(m)},\Bbb B\leftarrow L^{(m)},$\;
    $x\leftarrow \min{(a_i,b_i)}, y\leftarrow \max{(a_i,b_i)}, i \in (1,n)$\;
    The exponential transformation of matrices $\Bbb A\ and\ \Bbb B$\;
      $a_{ij}^{'}={(n+1)}^{y-a_{ij}}$\;
      $b_{ij}^{'}={(n+1)}^{y-b_{ij}}$\;
    \eIf{$L^{(m)}$ is sparse}{
      $\odot\leftarrow \bigcirc$//sparse matrix multiplication\;
    }{
      $\odot\leftarrow \otimes$//dense matrix multiplication\;
    }
    Given matrices $\Bbb C^{'}$, $\Bbb C^{'} \in \Bbb R^{n\times n}$\;
    $\Bbb C^{'}\leftarrow \Bbb A^{'}\odot\Bbb B^{'}$\;
    The logrithm transformation of matrix $\Bbb C^{'}$\;
    $\Bbb C\leftarrow \log{\Bbb C^{'}}$\;
    \If{$L^{(m)} = \Bbb C$}{
      {\bf return} $L^{(m)}$\;
    }
    $L^{(2m)}\leftarrow \Bbb C$\;
    $m\leftarrow 2\times m$\;
  }
  {\bf return} $L^{(m)}$\;
\end{algorithm}


\section{Architecture Design}

\subsection{Sparseness Judgment}

As shown in Table (1), observed the adjacent matrix represented by the network diagram is usually a sparse matrix, and the matrix gradually becomes dense during iterations,
and one idea of optimization is to use sparse matrix multiplication to speed up the iteration when the matrix is sparse, and when the matrix becomes denser, the sparse matrix multiplication is much time-consuming.
Taking the actors' social network as an example, the non-zero elements of the adjacent matrix of 8508 nodes is 617958, and the percentage of non-zero elements in the matrix is 0.8536\%, which is a typical sparse matrix.
Consider the advantages of sparse matrix multiplication algorithm, a threshold for triggering sparse matrix multiplication is set,
and when the proportion of non-zero-value elements entered is less than the 10\% threshold, performs sparse matrix multiplication.

\subsection{Convergence Judgment}

For the networks with unknown diameters, the calculation results can be made using the convergence method, and for the calculation process of reaching convergence,
i.e. $L^{(n-1)}=L^{(n)}$, should be regarded as the program termination of iteration, and the calculation of the matrix product method is completed by the Equation (4).

\subsection{Algorithm with Joint Sparseness Judgment and Convergence Judgment}

Based on Alon \etal distance product algorithm, utilizing the sparseness judgment of the input matrix and the calculation result convergence judgment,
with the network conforming to the power-law distribution and the precision of floating point operation, and the name implies two characteristics of the limitation
of floating point precision on the scale-free social network and the computer hardware, the algorithm process flow is shown in Algorithm 1.

\section{Experiment}

\textbf{PowerLaw} refers to the network node degrees conforming to the characteristics that social networks power-law distribution,
and \textbf{Bound} refers to the precision limit for diameters of floating point arithmetic operations on hardware.
Firstly, compared to Floyd-Warshall algorithm implemented on CPU, the Alon \etal algorithm implementation with matrix multiplication of block cache optimization on GPU has a time performance improvement, see Table (1).
PowerLaw-Bound$(a\sim f)$ shows that using different libraries on CPU/GPU, the proposed algorithm has significant performance improvement over Alon \etal algorithm.


\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Algorithm	& Time Cost(sec) \\
\hline\hline
Floyd-Warshall~\cite{floyd1962algorithm,warshall1962theorem}  &	$10^6$ \\
Alon \etal~\cite{alon1997exponent}  &	594.7 \\
PowerLaw-Bound$(a)$ &	427.9 \\
PowerLaw-Bound$(b)$	&	328.4 \\
PowerLaw-Bound$(c)$	&	95.0 \\
PowerLaw-Bound$(d)$	&	45.0 \\
PowerLaw-Bound$(e)$	&	19.32 \\
PowerLaw-Bound$(f)$	&	15.98 \\
\hline
\end{tabular}
\end{center}
\caption{Performance comparison on different hardware. (a) CPU numpy library. (b) CPU scipy/numpy library. (c) GPU cuBLAS. (d) CPU openBLAS. (e) GPU CuPy. (f) GPU CuPy-sparse/CuPy.}
\end{table}


\section{Conclusion}

This paper discussed a novel algorithm in the all-pairs shortest path computation tasks, using distance product association law.
The time-bound of Alon \etal distance product algorithm with block matrix multiplication is \(O({2\log{n}n^3\over \Bbb B})\).
To strengthen the conditions, on the scale-free social networks, for the precision limit of floating point operation on hardware,
additional condition on the social network diameter is added to avoid overflow in the floating point operations,
the analysis showed that the algorithm has a lower time-bound \(O({14n^3\over \Bbb B})\).
With a hypothesis that all the social networks have limit diameters, I proposed a novel algorithm,
which leverages matrix sparseness judgment, and calculation convergence judgment.
Comparing to Alon \etal's algorithm, the experimental results show that \textbf{PowerLaw-Bound} algorithm
has improvement of 39\% on CPU and improvement of 36.2 times on GPU on the Chinese Movie Celebrity Network $2011\sim2015$ dataset.

{\small
\bibliographystyle{ieee_fullname}
%\bibliography{cpvr}
\begin{thebibliography}{10}


\bibitem{alon1997exponent}
Noga Alon, Zvi Galil, and Oded Margalit.
\newblock On the exponent of the all pairs shortest path problem.
\newblock {\em Journal of Computer and System Sciences}, 54(2):255--262, 1997.


\bibitem{floyd1962algorithm}
Robert~W Floyd.
\newblock Algorithm 97: shortest path.
\newblock {\em Communications of the ACM}, 5(6):345, 1962.


\bibitem{warshall1962theorem}
Stephen Warshall.
\newblock A theorem on boolean matrices.
\newblock {\em Journal of the ACM (JACM)}, 9(1):11--12, 1962.

\bibitem{aho1974design}
Alfred~V Aho and John~E Hopcroft.
\newblock {\em The design and analysis of computer algorithms}.
\newblock Pearson Education India, 1974.


\bibitem{zwick2002all}
Uri Zwick.
\newblock All pairs shortest paths using bridging sets and rectangular matrix
  multiplication.
\newblock {\em Journal of the ACM (JACM)}, 49(3):289--317, 2002.


\bibitem{garbow1985scaling}
Harold~N Garbow.
\newblock Scaling algorithms for network problems.
\newblock {\em Journal of Computer and System Sciences}, 31(2):148--168, 1985.


\bibitem{strassen1969gaussian}
Volker Strassen.
\newblock Gaussian elimination is not optimal.
\newblock {\em Numerische mathematik}, 13(4):354--356, 1969.


\bibitem{coppersmith1987matrix}
Don Coppersmith and Shmuel Winograd.
\newblock Matrix multiplication via arithmetic progressions.
\newblock In {\em Proceedings of the nineteenth annual ACM symposium on Theory
  of computing}, pages 1--6, 1987.

\bibitem{davie2013improved}
Alexander~Munro Davie and Andrew~James Stothers.
\newblock Improved bound for complexity of matrix multiplication.
\newblock {\em Proceedings. Section A, Mathematics-The Royal Society of
  Edinburgh}, 143(2):351, 2013.

\bibitem{le2014powers}
Fran{\c{c}}ois Le~Gall.
\newblock Powers of tensors and fast matrix multiplication.
\newblock In {\em Proceedings of the 39th international symposium on symbolic
  and algebraic computation}, pages 296--303, 2014.


\bibitem{lam1991cache}
Monica~D Lam, Edward~E Rothberg, and Michael~E Wolf.
\newblock The cache performance and optimizations of blocked algorithms.
\newblock {\em ACM SIGOPS Operating Systems Review}, 25(Special Issue):63--74,
  1991.


\bibitem{goto2008anatomy}
Kazushige Goto and Robert A van~de Geijn.
\newblock Anatomy of high-performance matrix multiplication.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)}, 34(3):1--25,
  2008.


\bibitem{albert1999diameter}
R{\'e}ka Albert, Hawoong Jeong, and Albert-L{\'a}szl{\'o} Barab{\'a}si.
\newblock Diameter of the world-wide web.
\newblock {\em nature}, 401(6749):130--131, 1999.

\bibitem{ieee1985ieee}
IEEE Computer Society.~Standards Committee and American National~Standards
  Institute.
\newblock {\em IEEE standard for binary floating-point arithmetic}, volume 754.
\newblock IEEE, 1985.

\end{thebibliography}

}

\end{document}
